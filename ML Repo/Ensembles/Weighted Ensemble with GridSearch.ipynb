{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8896969696969697, 0.8648484848484849, 0.8812121212121212]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate a weighted average ensemble for classification compared to base model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# get a list of base models\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(('lr', LogisticRegression()))\n",
    "    models.append(('cart', DecisionTreeClassifier()))\n",
    "    models.append(('bayes', GaussianNB()))\n",
    "    return models\n",
    "# evaluate each base model\n",
    "def evaluate_models(models, X_train, X_val, y_train, y_val):\n",
    "    # fit and evaluate the models\n",
    "    scores = list()\n",
    "    for name, model in models:\n",
    "        # fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        # evaluate the model\n",
    "        yhat = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, yhat)\n",
    "        # store the performance\n",
    "        scores.append(acc)\n",
    "        # report model performance\n",
    "    return scores\n",
    "# generate grid for weights\n",
    "def generate_combinations(num_dimensions, step):\n",
    "    # Define the ranges for each dimension (0 to 1 in steps of 'step')\n",
    "    ranges = np.arange(0, 1 + step, step)\n",
    "    # Create a meshgrid of all combinations\n",
    "    meshgrid = np.meshgrid(*[ranges] * num_dimensions)\n",
    "    # Stack the grid points into a single array\n",
    "    combinations = np.vstack([x.flatten() for x in meshgrid]).T\n",
    "    # Remove the combination (0, 0, 0)\n",
    "    combinations = combinations[~np.all(combinations == 0, axis=1)]\n",
    "    # Filter out combinations where the sum is not exactly 1\n",
    "    valid_combinations = []\n",
    "    for combo in combinations:\n",
    "        if np.isclose(np.sum(combo), 1.0):\n",
    "            valid_combinations.append(combo)\n",
    "    \n",
    "    return np.array(valid_combinations)\n",
    "# for more granular search, make the step parameter closer\n",
    "combinations = generate_combinations(num_dimensions=3, step=0.1)\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# split dataset into train and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n",
    "# split the full train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n",
    "# create the base models\n",
    "models = get_models()\n",
    "# fit and evaluate each model\n",
    "scores = evaluate_models(models, X_train, X_val, y_train, y_val)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name     [0.4, 0.4, 0.2]6/66\n",
      "score             0.9092\n",
      "Name: 42, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Grid Searching different model weights\n",
    "ensemble_score = []\n",
    "i = 1\n",
    "for i, comb in enumerate(combinations, start=1):\n",
    "    print(f\"Processing combination {i}/{len(combinations)}\", end=\"\\r\")\n",
    "    weights = comb\n",
    "    ensemble = VotingClassifier(estimators=models, voting='soft', weights=weights)\n",
    "    # fit the ensemble on the training dataset\n",
    "    ensemble.fit(X_train_full, y_train_full)\n",
    "    # make predictions on test set\n",
    "    y_pred = ensemble.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    score = accuracy_score(y_test, y_pred)   \n",
    "    ensemble_score.append(score)\n",
    "df_scores = pd.DataFrame({'name': list(combinations), 'score': ensemble_score,})\n",
    "print(df_scores.sort_values(by='score', ascending=False).iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">lr: 87.800\n",
      ">cart: 88.700\n",
      ">bayes: 87.300\n",
      "Voting Accuracy: 90.720\n"
     ]
    }
   ],
   "source": [
    "# evaluate each standalone model\n",
    "scores = evaluate_models(models, X_train_full, X_test, y_train_full, y_test)\n",
    "for i in range(len(models)):\n",
    "    print('>%s: %.3f' % (models[i][0], scores[i]*100))\n",
    "# evaluate equal weighting\n",
    "ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "ensemble.fit(X_train_full, y_train_full)\n",
    "y_pred = ensemble.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print('Voting Accuracy: %.3f' % (score*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
