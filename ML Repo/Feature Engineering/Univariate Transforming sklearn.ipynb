{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preparing Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import*\n",
    "# Load the rock mines dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "df = pd.read_csv(url)\n",
    "column_names = [\"sensor_\" + str(i) for i in range(1, 61)] + [\"target\"]\n",
    "df.columns = column_names\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={\"index\": \"id\"})\n",
    "df['target'] = df['target'].map({'M': 1, 'R': 0})\n",
    "x_data = df.iloc[:, :60]\n",
    "x_data = x_data.drop('id', axis=1)\n",
    "y_data = df['target']\n",
    "x_data['encoded_column'] = (x_data['sensor_2'] < 0.02).astype(int)\n",
    "\n",
    "\n",
    "def apply_transformer_and_suffix(dataframe, transformer, suffix, **kwargs):\n",
    "    # Identify continuous columns (float or int data type)\n",
    "    continuous_columns = dataframe.select_dtypes(include=['float64']).columns\n",
    "    \n",
    "    # Create an instance of the specified transformer with additional parameters\n",
    "    scaler = transformer(**kwargs)\n",
    "    scaled_data = scaler.fit_transform(dataframe[continuous_columns])\n",
    "    \n",
    "    # Create new column names with the specified suffix\n",
    "    new_columns = [col + f\"_{suffix}\" for col in continuous_columns]\n",
    "    \n",
    "    # Create a new DataFrame with transformed data and modified column names\n",
    "    transformed_df = pd.DataFrame(scaled_data, columns=new_columns)\n",
    "    \n",
    "    # Combine transformed continuous columns with non-continuous columns\n",
    "    non_continuous_columns = dataframe.drop(columns=continuous_columns)\n",
    "    final_df = pd.concat([non_continuous_columns, transformed_df], axis=1)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "StandardScaler_df = apply_transformer_and_suffix(x_data, StandardScaler, \"StandardScaler\")\n",
    "Normalizer_df = apply_transformer_and_suffix(x_data, Normalizer, \"Normalizer\")\n",
    "MaxAbsScaler_df = apply_transformer_and_suffix(x_data, MaxAbsScaler, \"MaxAbsScaler\")\n",
    "MinMaxScaler_df = apply_transformer_and_suffix(x_data, MinMaxScaler, \"MinMaxScaler\")\n",
    "QuantileTransformer_df = apply_transformer_and_suffix(x_data, QuantileTransformer, \"QuantileTransformer\", n_quantiles=10)\n",
    "KBinsDiscretizer_df = apply_transformer_and_suffix(x_data, KBinsDiscretizer, \"KBinsDiscretizer\",n_bins=10, encode='ordinal', strategy='uniform')\n",
    "PowerTransformer_df = apply_transformer_and_suffix(x_data, PowerTransformer, \"PowerTransformer\",method='yeo-johnson', standardize=True)\n",
    "RobustScaler_df = apply_transformer_and_suffix(x_data, RobustScaler, \"RobustScaler\",with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
    "\n",
    "\n",
    "all_dfs = [StandardScaler_df, Normalizer_df, MaxAbsScaler_df, MinMaxScaler_df,QuantileTransformer_df,KBinsDiscretizer_df,PowerTransformer_df,RobustScaler_df]\n",
    "# Initialize an empty DataFrame to store the merged result\n",
    "merged_df = pd.DataFrame()\n",
    "# Concatenate DataFrames vertically, avoiding duplicate columns\n",
    "for df in all_dfs:\n",
    "    new_columns = [col for col in df.columns if col not in merged_df.columns]\n",
    "    merged_df = pd.concat([merged_df, df[new_columns]], axis=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
